# PRISM-Statistical_Analysis

# Confusion Matrix
True Positive (TP)
True Negative (TF)
False Positive (TP)
False Negative (TF)

# Accuracy
Overall correctness -- taking into account of both precision and recall

# PrecisionÂ 
Measures the quality of positive predictions, or how many of the predicted positive instances were actually positive. A high precision score means the model has a low rate of false positives.
The percentage of positives out of all predicted positives. For a perfect model, precision should be 1.
Precision = TP/(TP + FP)

# Recall
Assesses sensitivity to positive instances.
The percentage of correct positive predictions by the model out of all actual positives. For a perfect model, recall should be 1.
Recall = TP/(TP + FN)

# F-1 Score
A balanced measure that combines precision and recall to evaluate a model's accuracy. The closer the value is to 1, the better the model.
F-1 Score = (2 Precision Recall) / (Precision + Recall)
